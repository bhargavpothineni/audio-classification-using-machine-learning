{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512b6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "df='audiodata/dog_bark.wav'\n",
    "librosa_audio_data,librosa_sample_rate=librosa.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d260d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.         ...  0.00037083  0.00025414\n",
      " -0.00020702]\n"
     ]
    }
   ],
   "source": [
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c5ef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x128f083c2b0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce910f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile as wav\n",
    "wave_sample_rate, wave_audio = wav.read(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deab68b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0],\n",
       "       [  0,   0],\n",
       "       [  0,   0],\n",
       "       ...,\n",
       "       [  6,  -5],\n",
       "       [  7, -17],\n",
       "       [ -5, -21]], dtype=int16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f75589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a18e87a490>,\n",
       " <matplotlib.lines.Line2D at 0x1a18e87a4f0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89ba966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 173)\n"
     ]
    }
   ],
   "source": [
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8008e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.8003693e+02, -4.9177695e+02, -3.6617474e+02, ...,\n",
       "        -5.0730090e+02, -5.1229175e+02, -5.2267603e+02],\n",
       "       [ 3.0879448e+01,  1.1654912e+02,  1.7551926e+02, ...,\n",
       "         9.0842987e+01,  9.2580048e+01,  8.8495163e+01],\n",
       "       [ 1.7225266e+01,  3.9759499e+01, -5.0101862e+00, ...,\n",
       "         2.7333733e+01,  2.7949665e+01,  3.1583309e+01],\n",
       "       ...,\n",
       "       [-3.7395465e+00, -4.9923849e+00,  4.4441853e+00, ...,\n",
       "         2.7369006e+00,  1.6080487e+00,  2.7040436e+00],\n",
       "       [-1.9384034e+00, -4.6505064e-01,  6.2187910e+00, ...,\n",
       "         2.7966819e+00,  2.2690477e+00,  9.9589801e-01],\n",
       "       [ 1.7400664e+00,  2.0404918e+00,  4.3179483e+00, ...,\n",
       "         1.5787574e+00,  1.0260875e+00, -3.2637908e+00]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b46a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "df_audio='audiodata'\n",
    "d1=pd.read_csv('audiodata/UrbanSound8K.csv')\n",
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bdbbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693f8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3554it [03:47, 15.05it/s]C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  return f(*args, **kwargs)\n",
      "8324it [11:15, 13.74it/s]C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  return f(*args, **kwargs)\n",
      "8328it [11:15, 18.58it/s]C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  return f(*args, **kwargs)\n",
      "8732it [11:52, 12.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(d1.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(df_audio),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc74545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-217.35526, 70.22338, -130.38527, -53.282898,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-424.09818, 109.34077, -52.919525, 60.86475, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-458.79114, 121.38419, -46.520657, 52.00812, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-413.89984, 101.66373, -35.42945, 53.036354, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-446.60352, 113.68541, -52.402206, 60.302044,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-217.35526, 70.22338, -130.38527, -53.282898,...          dog_bark\n",
       "1  [-424.09818, 109.34077, -52.919525, 60.86475, ...  children_playing\n",
       "2  [-458.79114, 121.38419, -46.520657, 52.00812, ...  children_playing\n",
       "3  [-413.89984, 101.66373, -35.42945, 53.036354, ...  children_playing\n",
       "4  [-446.60352, 113.68541, -52.402206, 60.302044,...  children_playing"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d82b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "068b4c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b93c666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dog_bark', 'children_playing', 'children_playing', ...,\n",
       "       'car_horn', 'car_horn', 'car_horn'], dtype='<U16')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e3f920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de7f38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ddf7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c43b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.31104706e+02,  1.12505905e+02, -2.25746956e+01, ...,\n",
       "         3.24665260e+00, -1.36902368e+00,  2.75575471e+00],\n",
       "       [-1.36703424e+01,  9.10850830e+01, -7.79273319e+00, ...,\n",
       "        -3.25305033e+00, -5.27745247e+00, -1.55697143e+00],\n",
       "       [-4.98715439e+01,  2.65352994e-01, -2.05009365e+01, ...,\n",
       "         2.85459447e+00, -1.60920465e+00,  3.52480578e+00],\n",
       "       ...,\n",
       "       [-4.27012360e+02,  9.26230469e+01,  3.12939739e+00, ...,\n",
       "         7.42641389e-01,  7.33490825e-01,  7.11009085e-01],\n",
       "       [-1.45754608e+02,  1.36265778e+02, -3.35155220e+01, ...,\n",
       "         1.46811950e+00, -2.00917006e+00, -8.82181883e-01],\n",
       "       [-4.21031342e+02,  2.10654541e+02,  3.49066067e+00, ...,\n",
       "        -5.38886690e+00, -3.37136054e+00, -1.56651139e+00]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76bd1152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eec4584e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5937, 40)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25851703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33e82426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5937, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "499b47fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4af1bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_val, y_train, y_val = train_test_split(X_train,y_train,random_state = 6, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fd5f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Dropout\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13e342d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c67d86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import gc\n",
    "import pickle\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from numpy import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4c21e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(padding = 'same', filters = 10, kernel_size = 4, strides = 2, input_shape = (16000, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "for i in range(9):\n",
    "    model.add(layers.Conv1D(padding = 'same', filters = 10, kernel_size = 4, strides = 2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49d3e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60d8ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"audiodata\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f2b9d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 8000, 10)          50        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8000, 10)         40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 8000, 10)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4000, 10)          410       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4000, 10)         40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 4000, 10)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 2000, 10)          410       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2000, 10)         40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2000, 10)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1000, 10)          410       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1000, 10)         40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1000, 10)          0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 500, 10)           410       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 500, 10)          40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 500, 10)           0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 250, 10)           410       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 250, 10)          40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 250, 10)           0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 125, 10)           410       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 125, 10)          40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 125, 10)           0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 63, 10)            410       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 63, 10)           40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 63, 10)            0         \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 32, 10)            410       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 32, 10)           40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 32, 10)            0         \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 16, 10)            410       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 16, 10)           40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 16, 10)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 10)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,151\n",
      "Trainable params: 3,951\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fddbd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df149437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "985eccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5fd1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a4f42a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 100)               4100      \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,410\n",
      "Trainable params: 45,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a968560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d6d1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    nclass = len(df)\n",
    "    inp = Input(shape=(input_shape, 1))\n",
    "    img_1 = Convolution1D(16, kernel_size=9, activation=activations.relu, padding=\"valid\")(inp)\n",
    "    img_1 = Convolution1D(16, kernel_size=9, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=16)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=4)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=4)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "    dense_1 = Dense(64, activation=activations.relu)(img_1)\n",
    "    dense_1 = Dense(1028, activation=activations.relu)(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.0001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30419337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "139/158 [=========================>....] - ETA: 0s - loss: 13.2469 - accuracy: 0.1263\n",
      "Epoch 1: val_loss improved from inf to 2.28519, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 12ms/step - loss: 12.1174 - accuracy: 0.1278 - val_loss: 2.2852 - val_accuracy: 0.0933\n",
      "Epoch 2/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 2.8118 - accuracy: 0.1250\n",
      "Epoch 2: val_loss did not improve from 2.28519\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 2.8093 - accuracy: 0.1247 - val_loss: 2.2879 - val_accuracy: 0.1133\n",
      "Epoch 3/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 2.3926 - accuracy: 0.1216\n",
      "Epoch 3: val_loss improved from 2.28519 to 2.28059, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.3919 - accuracy: 0.1213 - val_loss: 2.2806 - val_accuracy: 0.1088\n",
      "Epoch 4/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 2.3348 - accuracy: 0.1273\n",
      "Epoch 4: val_loss improved from 2.28059 to 2.27481, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.3346 - accuracy: 0.1264 - val_loss: 2.2748 - val_accuracy: 0.1070\n",
      "Epoch 5/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 2.2798 - accuracy: 0.1272\n",
      "Epoch 5: val_loss improved from 2.27481 to 2.25589, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 2.2801 - accuracy: 0.1268 - val_loss: 2.2559 - val_accuracy: 0.1082\n",
      "Epoch 6/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 2.2454 - accuracy: 0.1346\n",
      "Epoch 6: val_loss improved from 2.25589 to 2.22027, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 2.2450 - accuracy: 0.1348 - val_loss: 2.2203 - val_accuracy: 0.1694\n",
      "Epoch 7/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 2.2157 - accuracy: 0.1672\n",
      "Epoch 7: val_loss improved from 2.22027 to 2.16388, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.2123 - accuracy: 0.1683 - val_loss: 2.1639 - val_accuracy: 0.1935\n",
      "Epoch 8/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 2.1991 - accuracy: 0.1747\n",
      "Epoch 8: val_loss improved from 2.16388 to 2.14523, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 2.2010 - accuracy: 0.1736 - val_loss: 2.1452 - val_accuracy: 0.1981\n",
      "Epoch 9/100\n",
      "143/158 [==========================>...] - ETA: 0s - loss: 2.1862 - accuracy: 0.1818\n",
      "Epoch 9: val_loss improved from 2.14523 to 2.09475, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.1842 - accuracy: 0.1813 - val_loss: 2.0947 - val_accuracy: 0.2169\n",
      "Epoch 10/100\n",
      "140/158 [=========================>....] - ETA: 0s - loss: 2.1456 - accuracy: 0.1935\n",
      "Epoch 10: val_loss improved from 2.09475 to 2.06839, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 12ms/step - loss: 2.1451 - accuracy: 0.1918 - val_loss: 2.0684 - val_accuracy: 0.2141\n",
      "Epoch 11/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 2.1292 - accuracy: 0.1961\n",
      "Epoch 11: val_loss improved from 2.06839 to 2.05084, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.1278 - accuracy: 0.1972 - val_loss: 2.0508 - val_accuracy: 0.2187\n",
      "Epoch 12/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 2.0985 - accuracy: 0.2082\n",
      "Epoch 12: val_loss improved from 2.05084 to 2.03512, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.0999 - accuracy: 0.2077 - val_loss: 2.0351 - val_accuracy: 0.2227\n",
      "Epoch 13/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 2.0813 - accuracy: 0.2161\n",
      "Epoch 13: val_loss improved from 2.03512 to 1.99144, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 2.0814 - accuracy: 0.2168 - val_loss: 1.9914 - val_accuracy: 0.2456\n",
      "Epoch 14/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 2.0503 - accuracy: 0.2395\n",
      "Epoch 14: val_loss improved from 1.99144 to 1.95920, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 2.0496 - accuracy: 0.2396 - val_loss: 1.9592 - val_accuracy: 0.2891\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 2.0366 - accuracy: 0.2374\n",
      "Epoch 15: val_loss did not improve from 1.95920\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2.0366 - accuracy: 0.2374 - val_loss: 1.9626 - val_accuracy: 0.2977\n",
      "Epoch 16/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 2.0178 - accuracy: 0.2608\n",
      "Epoch 16: val_loss improved from 1.95920 to 1.94112, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 2.0136 - accuracy: 0.2608 - val_loss: 1.9411 - val_accuracy: 0.3148\n",
      "Epoch 17/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.9768 - accuracy: 0.2707\n",
      "Epoch 17: val_loss improved from 1.94112 to 1.85909, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.9737 - accuracy: 0.2721 - val_loss: 1.8591 - val_accuracy: 0.3480\n",
      "Epoch 18/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 1.9468 - accuracy: 0.2829\n",
      "Epoch 18: val_loss improved from 1.85909 to 1.82480, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.9465 - accuracy: 0.2824 - val_loss: 1.8248 - val_accuracy: 0.3532\n",
      "Epoch 19/100\n",
      "145/158 [==========================>...] - ETA: 0s - loss: 1.9194 - accuracy: 0.2862\n",
      "Epoch 19: val_loss improved from 1.82480 to 1.78288, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.9212 - accuracy: 0.2850 - val_loss: 1.7829 - val_accuracy: 0.3732\n",
      "Epoch 20/100\n",
      "143/158 [==========================>...] - ETA: 0s - loss: 1.8994 - accuracy: 0.2994\n",
      "Epoch 20: val_loss did not improve from 1.78288\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.9006 - accuracy: 0.2961 - val_loss: 1.7833 - val_accuracy: 0.3698\n",
      "Epoch 21/100\n",
      "136/158 [========================>.....] - ETA: 0s - loss: 1.8649 - accuracy: 0.3125\n",
      "Epoch 21: val_loss improved from 1.78288 to 1.72675, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.8575 - accuracy: 0.3145 - val_loss: 1.7268 - val_accuracy: 0.3978\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.8226 - accuracy: 0.3379\n",
      "Epoch 22: val_loss improved from 1.72675 to 1.67346, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 1.8226 - accuracy: 0.3379 - val_loss: 1.6735 - val_accuracy: 0.4001\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.8042 - accuracy: 0.3482\n",
      "Epoch 23: val_loss improved from 1.67346 to 1.64454, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.8042 - accuracy: 0.3482 - val_loss: 1.6445 - val_accuracy: 0.4201\n",
      "Epoch 24/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.7711 - accuracy: 0.3649\n",
      "Epoch 24: val_loss improved from 1.64454 to 1.59698, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.7702 - accuracy: 0.3644 - val_loss: 1.5970 - val_accuracy: 0.4545\n",
      "Epoch 25/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.7218 - accuracy: 0.3877\n",
      "Epoch 25: val_loss improved from 1.59698 to 1.55089, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.7222 - accuracy: 0.3870 - val_loss: 1.5509 - val_accuracy: 0.4677\n",
      "Epoch 26/100\n",
      "145/158 [==========================>...] - ETA: 0s - loss: 1.7056 - accuracy: 0.3942\n",
      "Epoch 26: val_loss improved from 1.55089 to 1.50470, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.6978 - accuracy: 0.3960 - val_loss: 1.5047 - val_accuracy: 0.4699\n",
      "Epoch 27/100\n",
      "147/158 [==========================>...] - ETA: 0s - loss: 1.6613 - accuracy: 0.4128\n",
      "Epoch 27: val_loss improved from 1.50470 to 1.48853, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.6607 - accuracy: 0.4148 - val_loss: 1.4885 - val_accuracy: 0.4848\n",
      "Epoch 28/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.6120 - accuracy: 0.4178\n",
      "Epoch 28: val_loss improved from 1.48853 to 1.43366, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 12ms/step - loss: 1.6103 - accuracy: 0.4184 - val_loss: 1.4337 - val_accuracy: 0.5175\n",
      "Epoch 29/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.6046 - accuracy: 0.4402\n",
      "Epoch 29: val_loss improved from 1.43366 to 1.41535, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.6054 - accuracy: 0.4394 - val_loss: 1.4154 - val_accuracy: 0.5369\n",
      "Epoch 30/100\n",
      "147/158 [==========================>...] - ETA: 0s - loss: 1.5614 - accuracy: 0.4435\n",
      "Epoch 30: val_loss improved from 1.41535 to 1.39479, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.5621 - accuracy: 0.4455 - val_loss: 1.3948 - val_accuracy: 0.5392\n",
      "Epoch 31/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 1.5292 - accuracy: 0.4566\n",
      "Epoch 31: val_loss improved from 1.39479 to 1.36151, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 1.5275 - accuracy: 0.4566 - val_loss: 1.3615 - val_accuracy: 0.5432\n",
      "Epoch 32/100\n",
      "144/158 [==========================>...] - ETA: 0s - loss: 1.5066 - accuracy: 0.4787\n",
      "Epoch 32: val_loss improved from 1.36151 to 1.34735, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.4980 - accuracy: 0.4786 - val_loss: 1.3473 - val_accuracy: 0.5547\n",
      "Epoch 33/100\n",
      "143/158 [==========================>...] - ETA: 0s - loss: 1.4876 - accuracy: 0.4749\n",
      "Epoch 33: val_loss did not improve from 1.34735\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.4856 - accuracy: 0.4774 - val_loss: 1.3485 - val_accuracy: 0.5426\n",
      "Epoch 34/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 1.4484 - accuracy: 0.4806\n",
      "Epoch 34: val_loss improved from 1.34735 to 1.31570, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 10ms/step - loss: 1.4570 - accuracy: 0.4790 - val_loss: 1.3157 - val_accuracy: 0.5713\n",
      "Epoch 35/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.4384 - accuracy: 0.4932\n",
      "Epoch 35: val_loss improved from 1.31570 to 1.26808, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.4377 - accuracy: 0.4915 - val_loss: 1.2681 - val_accuracy: 0.6062\n",
      "Epoch 36/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.4364 - accuracy: 0.4942\n",
      "Epoch 36: val_loss improved from 1.26808 to 1.24431, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 1.4309 - accuracy: 0.4962 - val_loss: 1.2443 - val_accuracy: 0.5953\n",
      "Epoch 37/100\n",
      "140/158 [=========================>....] - ETA: 0s - loss: 1.4169 - accuracy: 0.5013\n",
      "Epoch 37: val_loss improved from 1.24431 to 1.24251, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.4118 - accuracy: 0.5050 - val_loss: 1.2425 - val_accuracy: 0.6033\n",
      "Epoch 38/100\n",
      "147/158 [==========================>...] - ETA: 0s - loss: 1.3825 - accuracy: 0.5193\n",
      "Epoch 38: val_loss improved from 1.24251 to 1.21513, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 1.3863 - accuracy: 0.5161 - val_loss: 1.2151 - val_accuracy: 0.6085\n",
      "Epoch 39/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 1.3648 - accuracy: 0.5211\n",
      "Epoch 39: val_loss improved from 1.21513 to 1.20574, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.3693 - accuracy: 0.5218 - val_loss: 1.2057 - val_accuracy: 0.6068\n",
      "Epoch 40/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.3706 - accuracy: 0.5225\n",
      "Epoch 40: val_loss did not improve from 1.20574\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.3700 - accuracy: 0.5226 - val_loss: 1.2237 - val_accuracy: 0.5919\n",
      "Epoch 41/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.3656 - accuracy: 0.5198\n",
      "Epoch 41: val_loss improved from 1.20574 to 1.19693, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.3634 - accuracy: 0.5212 - val_loss: 1.1969 - val_accuracy: 0.6096\n",
      "Epoch 42/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 1.3350 - accuracy: 0.5329\n",
      "Epoch 42: val_loss improved from 1.19693 to 1.18767, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.3349 - accuracy: 0.5327 - val_loss: 1.1877 - val_accuracy: 0.6211\n",
      "Epoch 43/100\n",
      "142/158 [=========================>....] - ETA: 0s - loss: 1.3071 - accuracy: 0.5460\n",
      "Epoch 43: val_loss improved from 1.18767 to 1.14584, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.3036 - accuracy: 0.5456 - val_loss: 1.1458 - val_accuracy: 0.6422\n",
      "Epoch 44/100\n",
      "146/158 [==========================>...] - ETA: 0s - loss: 1.3081 - accuracy: 0.5486\n",
      "Epoch 44: val_loss did not improve from 1.14584\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.3042 - accuracy: 0.5486 - val_loss: 1.1608 - val_accuracy: 0.6365\n",
      "Epoch 45/100\n",
      "141/158 [=========================>....] - ETA: 0s - loss: 1.2871 - accuracy: 0.5490\n",
      "Epoch 45: val_loss improved from 1.14584 to 1.14433, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 1.2848 - accuracy: 0.5517 - val_loss: 1.1443 - val_accuracy: 0.6308\n",
      "Epoch 46/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.2818 - accuracy: 0.5584\n",
      "Epoch 46: val_loss improved from 1.14433 to 1.12592, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.2845 - accuracy: 0.5600 - val_loss: 1.1259 - val_accuracy: 0.6525\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.2632 - accuracy: 0.5612\n",
      "Epoch 47: val_loss did not improve from 1.12592\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.2632 - accuracy: 0.5612 - val_loss: 1.1400 - val_accuracy: 0.6405\n",
      "Epoch 48/100\n",
      "145/158 [==========================>...] - ETA: 0s - loss: 1.2710 - accuracy: 0.5688\n",
      "Epoch 48: val_loss improved from 1.12592 to 1.09907, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.2740 - accuracy: 0.5658 - val_loss: 1.0991 - val_accuracy: 0.6377\n",
      "Epoch 49/100\n",
      "144/158 [==========================>...] - ETA: 0s - loss: 1.2565 - accuracy: 0.5712\n",
      "Epoch 49: val_loss did not improve from 1.09907\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.2513 - accuracy: 0.5715 - val_loss: 1.1256 - val_accuracy: 0.6325\n",
      "Epoch 50/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.2391 - accuracy: 0.5713\n",
      "Epoch 50: val_loss improved from 1.09907 to 1.06091, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.2387 - accuracy: 0.5721 - val_loss: 1.0609 - val_accuracy: 0.6560\n",
      "Epoch 51/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 1.2093 - accuracy: 0.5760\n",
      "Epoch 51: val_loss did not improve from 1.06091\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.2150 - accuracy: 0.5739 - val_loss: 1.0730 - val_accuracy: 0.6491\n",
      "Epoch 52/100\n",
      "143/158 [==========================>...] - ETA: 0s - loss: 1.2107 - accuracy: 0.5804\n",
      "Epoch 52: val_loss improved from 1.06091 to 1.05700, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 1.2072 - accuracy: 0.5807 - val_loss: 1.0570 - val_accuracy: 0.6577\n",
      "Epoch 53/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.2461 - accuracy: 0.5775\n",
      "Epoch 53: val_loss did not improve from 1.05700\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.2482 - accuracy: 0.5771 - val_loss: 1.0675 - val_accuracy: 0.6617\n",
      "Epoch 54/100\n",
      "146/158 [==========================>...] - ETA: 0s - loss: 1.1971 - accuracy: 0.5899\n",
      "Epoch 54: val_loss did not improve from 1.05700\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1.2062 - accuracy: 0.5884 - val_loss: 1.1060 - val_accuracy: 0.6463\n",
      "Epoch 55/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.2005 - accuracy: 0.5882\n",
      "Epoch 55: val_loss improved from 1.05700 to 1.05404, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 12ms/step - loss: 1.1976 - accuracy: 0.5878 - val_loss: 1.0540 - val_accuracy: 0.6606\n",
      "Epoch 56/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.1868 - accuracy: 0.5897\n",
      "Epoch 56: val_loss improved from 1.05404 to 1.03223, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 12ms/step - loss: 1.1879 - accuracy: 0.5898 - val_loss: 1.0322 - val_accuracy: 0.6714\n",
      "Epoch 57/100\n",
      "145/158 [==========================>...] - ETA: 0s - loss: 1.1926 - accuracy: 0.5920\n",
      "Epoch 57: val_loss improved from 1.03223 to 1.02344, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.1944 - accuracy: 0.5896 - val_loss: 1.0234 - val_accuracy: 0.6703\n",
      "Epoch 58/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.1745 - accuracy: 0.5944\n",
      "Epoch 58: val_loss did not improve from 1.02344\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1.1761 - accuracy: 0.5949 - val_loss: 1.0518 - val_accuracy: 0.6669\n",
      "Epoch 59/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.1667 - accuracy: 0.5942\n",
      "Epoch 59: val_loss did not improve from 1.02344\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1.1689 - accuracy: 0.5947 - val_loss: 1.0239 - val_accuracy: 0.6812\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.1552 - accuracy: 0.6040\n",
      "Epoch 60: val_loss improved from 1.02344 to 1.00709, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 13ms/step - loss: 1.1552 - accuracy: 0.6040 - val_loss: 1.0071 - val_accuracy: 0.6840\n",
      "Epoch 61/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.1424 - accuracy: 0.6061\n",
      "Epoch 61: val_loss improved from 1.00709 to 0.99008, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 13ms/step - loss: 1.1443 - accuracy: 0.6050 - val_loss: 0.9901 - val_accuracy: 0.6812\n",
      "Epoch 62/100\n",
      "144/158 [==========================>...] - ETA: 0s - loss: 1.1475 - accuracy: 0.6050\n",
      "Epoch 62: val_loss did not improve from 0.99008\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1.1420 - accuracy: 0.6070 - val_loss: 1.0197 - val_accuracy: 0.6743\n",
      "Epoch 63/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 1.1615 - accuracy: 0.6092\n",
      "Epoch 63: val_loss improved from 0.99008 to 0.98568, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 1.1614 - accuracy: 0.6064 - val_loss: 0.9857 - val_accuracy: 0.6800\n",
      "Epoch 64/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 1.1105 - accuracy: 0.6216\n",
      "Epoch 64: val_loss did not improve from 0.98568\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1.1124 - accuracy: 0.6213 - val_loss: 0.9876 - val_accuracy: 0.6817\n",
      "Epoch 65/100\n",
      "141/158 [=========================>....] - ETA: 0s - loss: 1.1323 - accuracy: 0.6124\n",
      "Epoch 65: val_loss did not improve from 0.98568\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1.1270 - accuracy: 0.6153 - val_loss: 1.0205 - val_accuracy: 0.6634\n",
      "Epoch 66/100\n",
      "146/158 [==========================>...] - ETA: 0s - loss: 1.1296 - accuracy: 0.6134\n",
      "Epoch 66: val_loss improved from 0.98568 to 0.97556, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.1278 - accuracy: 0.6141 - val_loss: 0.9756 - val_accuracy: 0.7001\n",
      "Epoch 67/100\n",
      "141/158 [=========================>....] - ETA: 0s - loss: 1.1403 - accuracy: 0.6177\n",
      "Epoch 67: val_loss did not improve from 0.97556\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.1365 - accuracy: 0.6159 - val_loss: 0.9773 - val_accuracy: 0.6852\n",
      "Epoch 68/100\n",
      "143/158 [==========================>...] - ETA: 0s - loss: 1.0998 - accuracy: 0.6292\n",
      "Epoch 68: val_loss improved from 0.97556 to 0.96232, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.0958 - accuracy: 0.6312 - val_loss: 0.9623 - val_accuracy: 0.6817\n",
      "Epoch 69/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.0933 - accuracy: 0.6194\n",
      "Epoch 69: val_loss did not improve from 0.96232\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0981 - accuracy: 0.6195 - val_loss: 0.9838 - val_accuracy: 0.6703\n",
      "Epoch 70/100\n",
      "145/158 [==========================>...] - ETA: 0s - loss: 1.1249 - accuracy: 0.6151\n",
      "Epoch 70: val_loss improved from 0.96232 to 0.95294, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.1254 - accuracy: 0.6159 - val_loss: 0.9529 - val_accuracy: 0.6955\n",
      "Epoch 71/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.1116 - accuracy: 0.6211\n",
      "Epoch 71: val_loss did not improve from 0.95294\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.1086 - accuracy: 0.6221 - val_loss: 0.9563 - val_accuracy: 0.6869\n",
      "Epoch 72/100\n",
      "140/158 [=========================>....] - ETA: 0s - loss: 1.1010 - accuracy: 0.6214\n",
      "Epoch 72: val_loss did not improve from 0.95294\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0867 - accuracy: 0.6270 - val_loss: 0.9707 - val_accuracy: 0.6920\n",
      "Epoch 73/100\n",
      "140/158 [=========================>....] - ETA: 0s - loss: 1.0825 - accuracy: 0.6214\n",
      "Epoch 73: val_loss did not improve from 0.95294\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1.0738 - accuracy: 0.6258 - val_loss: 0.9537 - val_accuracy: 0.6898\n",
      "Epoch 74/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.0878 - accuracy: 0.6242\n",
      "Epoch 74: val_loss did not improve from 0.95294\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1.0842 - accuracy: 0.6252 - val_loss: 0.9889 - val_accuracy: 0.6938\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.1184 - accuracy: 0.6292\n",
      "Epoch 75: val_loss improved from 0.95294 to 0.94494, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 1.1184 - accuracy: 0.6292 - val_loss: 0.9449 - val_accuracy: 0.7035\n",
      "Epoch 76/100\n",
      "139/158 [=========================>....] - ETA: 0s - loss: 1.0705 - accuracy: 0.6351\n",
      "Epoch 76: val_loss did not improve from 0.94494\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0674 - accuracy: 0.6359 - val_loss: 0.9488 - val_accuracy: 0.6920\n",
      "Epoch 77/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 1.0582 - accuracy: 0.6395\n",
      "Epoch 77: val_loss improved from 0.94494 to 0.93329, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 1.0572 - accuracy: 0.6393 - val_loss: 0.9333 - val_accuracy: 0.7058\n",
      "Epoch 78/100\n",
      "142/158 [=========================>....] - ETA: 0s - loss: 1.0680 - accuracy: 0.6356\n",
      "Epoch 78: val_loss improved from 0.93329 to 0.92729, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.0705 - accuracy: 0.6354 - val_loss: 0.9273 - val_accuracy: 0.7006\n",
      "Epoch 79/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.0838 - accuracy: 0.6394\n",
      "Epoch 79: val_loss did not improve from 0.92729\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0836 - accuracy: 0.6389 - val_loss: 0.9324 - val_accuracy: 0.7046\n",
      "Epoch 80/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.0471 - accuracy: 0.6424\n",
      "Epoch 80: val_loss did not improve from 0.92729\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0443 - accuracy: 0.6433 - val_loss: 0.9308 - val_accuracy: 0.7023\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.0464 - accuracy: 0.6383\n",
      "Epoch 81: val_loss improved from 0.92729 to 0.91707, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.0464 - accuracy: 0.6383 - val_loss: 0.9171 - val_accuracy: 0.7172\n",
      "Epoch 82/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.0790 - accuracy: 0.6393\n",
      "Epoch 82: val_loss did not improve from 0.91707\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0781 - accuracy: 0.6393 - val_loss: 0.9460 - val_accuracy: 0.6995\n",
      "Epoch 83/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 1.0750 - accuracy: 0.6393\n",
      "Epoch 83: val_loss improved from 0.91707 to 0.90324, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 1.0711 - accuracy: 0.6401 - val_loss: 0.9032 - val_accuracy: 0.7098\n",
      "Epoch 84/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.6450\n",
      "Epoch 84: val_loss did not improve from 0.90324\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0389 - accuracy: 0.6443 - val_loss: 0.9102 - val_accuracy: 0.7127\n",
      "Epoch 85/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.0537 - accuracy: 0.6440\n",
      "Epoch 85: val_loss improved from 0.90324 to 0.90223, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.0553 - accuracy: 0.6441 - val_loss: 0.9022 - val_accuracy: 0.7121\n",
      "Epoch 86/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.0472 - accuracy: 0.6482\n",
      "Epoch 86: val_loss improved from 0.90223 to 0.89208, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.0509 - accuracy: 0.6467 - val_loss: 0.8921 - val_accuracy: 0.7132\n",
      "Epoch 87/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 1.0453 - accuracy: 0.6476\n",
      "Epoch 87: val_loss did not improve from 0.89208\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0445 - accuracy: 0.6476 - val_loss: 0.8987 - val_accuracy: 0.7167\n",
      "Epoch 88/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.6491\n",
      "Epoch 88: val_loss did not improve from 0.89208\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0279 - accuracy: 0.6500 - val_loss: 0.9047 - val_accuracy: 0.7310\n",
      "Epoch 89/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.6456\n",
      "Epoch 89: val_loss did not improve from 0.89208\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0381 - accuracy: 0.6449 - val_loss: 0.9275 - val_accuracy: 0.7041\n",
      "Epoch 90/100\n",
      "144/158 [==========================>...] - ETA: 0s - loss: 1.0133 - accuracy: 0.6549\n",
      "Epoch 90: val_loss improved from 0.89208 to 0.89173, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.0155 - accuracy: 0.6520 - val_loss: 0.8917 - val_accuracy: 0.7247\n",
      "Epoch 91/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.0345 - accuracy: 0.6510\n",
      "Epoch 91: val_loss did not improve from 0.89173\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0391 - accuracy: 0.6506 - val_loss: 0.9274 - val_accuracy: 0.7092\n",
      "Epoch 92/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.0310 - accuracy: 0.6478\n",
      "Epoch 92: val_loss did not improve from 0.89173\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0311 - accuracy: 0.6486 - val_loss: 0.9128 - val_accuracy: 0.7144\n",
      "Epoch 93/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.0027 - accuracy: 0.6581\n",
      "Epoch 93: val_loss improved from 0.89173 to 0.86802, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 1.0104 - accuracy: 0.6562 - val_loss: 0.8680 - val_accuracy: 0.7184\n",
      "Epoch 94/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 0.9901 - accuracy: 0.6655\n",
      "Epoch 94: val_loss did not improve from 0.86802\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 0.9943 - accuracy: 0.6643 - val_loss: 0.8791 - val_accuracy: 0.7132\n",
      "Epoch 95/100\n",
      "146/158 [==========================>...] - ETA: 0s - loss: 1.0140 - accuracy: 0.6567\n",
      "Epoch 95: val_loss did not improve from 0.86802\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0124 - accuracy: 0.6581 - val_loss: 0.8766 - val_accuracy: 0.7367\n",
      "Epoch 96/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.0430 - accuracy: 0.6476\n",
      "Epoch 96: val_loss did not improve from 0.86802\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0417 - accuracy: 0.6482 - val_loss: 0.8709 - val_accuracy: 0.7224\n",
      "Epoch 97/100\n",
      "141/158 [=========================>....] - ETA: 0s - loss: 1.0357 - accuracy: 0.6487\n",
      "Epoch 97: val_loss improved from 0.86802 to 0.86727, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 1.0303 - accuracy: 0.6528 - val_loss: 0.8673 - val_accuracy: 0.7321\n",
      "Epoch 98/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 1.0029 - accuracy: 0.6641\n",
      "Epoch 98: val_loss improved from 0.86727 to 0.85953, saving model to audiodata\n",
      "INFO:tensorflow:Assets written to: audiodata\\assets\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.9999 - accuracy: 0.6637 - val_loss: 0.8595 - val_accuracy: 0.7264\n",
      "Epoch 99/100\n",
      "143/158 [==========================>...] - ETA: 0s - loss: 1.0197 - accuracy: 0.6567\n",
      "Epoch 99: val_loss did not improve from 0.85953\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0146 - accuracy: 0.6572 - val_loss: 0.8920 - val_accuracy: 0.7275\n",
      "Epoch 100/100\n",
      "146/158 [==========================>...] - ETA: 0s - loss: 1.0038 - accuracy: 0.6650\n",
      "Epoch 100: val_loss did not improve from 0.85953\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 1.0017 - accuracy: 0.6621 - val_loss: 0.8620 - val_accuracy: 0.7270\n",
      "Training completed in time:  0:02:00.222079\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='audiodata', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999b430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
